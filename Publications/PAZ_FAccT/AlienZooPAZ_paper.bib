
@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	pages = {2825--2830},
}

@misc{artelt_ceml_2019,
	title = {{CEML}: {Counterfactuals} for {Explaining} {Machine} {Learning} models - {A} {Python} toolbox},
	url = {https://www.github.com/andreArtelt/ceml},
	publisher = {GitHub},
	author = {Artelt, Andr{\'e}},
	year = {2019},
	note = {Publication Title: GitHub repository},
}

@article{wachter_counterfactual_2017,
	title = {Counterfactual explanations without opening the black box: {Automated} decisions and the {GDPR}},
	volume = {31},
	journal = {Harv. JL \& Tech.},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	year = {2017},
	note = {Publisher: HeinOnline},
	pages = {841},
}

@article{poyiadzi_face_2019,
	title = {{FACE}: {Feasible} and {Actionable} {Counterfactual} {Explanations}},
	volume = {abs/1909.09369},
	url = {http://arxiv.org/abs/1909.09369},
	journal = {CoRR},
	author = {Poyiadzi, Rafael and Sokol, Kacper and Santos-Rodriguez, Ra{\'u}l and Bie, Tijl De and Flach, Peter A.},
	year = {2019},
	note = {\_eprint: 1909.09369},
}

@article{looveren_interpretable_2019,
	title = {Interpretable {Counterfactual} {Explanations} {Guided} by {Prototypes}},
	volume = {abs/1907.02584},
	url = {http://arxiv.org/abs/1907.02584},
	journal = {CoRR},
	author = {Looveren, Arnaud Van and Klaise, Janis},
	year = {2019},
	note = {\_eprint: 1907.02584},
}

@article{artelt_computation_2019,
	title = {On the computation of counterfactual explanations - {A} survey},
	volume = {abs/1911.07749},
	url = {http://arxiv.org/abs/1911.07749},
	journal = {CoRR},
	author = {Artelt, Andr{\'e} and Hammer, Barbara},
	year = {2019},
	note = {\_eprint: 1911.07749},
}

@article{karimi_survey_2020,
	title = {A survey of algorithmic recourse: definitions, formulations, solutions, and prospects},
	journal = {arXiv preprint arXiv:2010.04050},
	author = {Karimi, Amir-Hossein and Barthe, Gilles and Sch{\"o}lkopf, Bernhard and Valera, Isabel},
	year = {2020},
}

@article{laugel_issues_2019,
	title = {Issues with post-hoc counterfactual explanations: a discussion},
	volume = {abs/1906.04774},
	url = {http://arxiv.org/abs/1906.04774},
	journal = {CoRR},
	author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Detyniecki, Marcin},
	year = {2019},
	note = {\_eprint: 1906.04774},
}

@article{byrne_counterfactual_2016,
	title = {Counterfactual {Thought}},
	volume = {67},
	issn = {0066-4308, 1545-2085},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-psych-122414-033249},
	doi = {10.1146/annurev-psych-122414-033249},
	abstract = {People spontaneously create counterfactual alternatives to reality when they think {\textquotedblleft}if only{\textquotedblright} or {\textquotedblleft}what if{\textquotedblright} and imagine how the past could have been different. The mind computes counterfactuals for many reasons. Counterfactuals explain the past and prepare for the future, they implicate various relations including causal ones, and they affect intentions and decisions. They modulate emotions such as regret and relief, and they support moral judgments such as blame. The loss of the ability to imagine alternatives as a result of injuries to the prefrontal cortex is devastating. The basic cognitive processes that compute counterfactuals mutate aspects of the mental representation of reality to create an imagined alternative, and they compare alternative representations. The ability to create counterfactuals develops throughout childhood and contributes to reasoning about other people{\textquoteright}s beliefs, including their false beliefs. Knowledge affects the plausibility of a counterfactual through the semantic and pragmatic modulation of the mental representation of alternative possibilities.},
	language = {en},
	number = {1},
	urldate = {2019-08-02},
	journal = {Annual Review of Psychology},
	author = {Byrne, Ruth M.J.},
	month = jan,
	year = {2016},
	pages = {135--157},
	file = {Byrne - 2016 - Counterfactual Thought.pdf:/Users/ukuhl/Zotero/storage/4Z9UMBH7/Byrne - 2016 - Counterfactual Thought.pdf:application/pdf},
}

@article{doshi-velez_towards_2017,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	language = {en},
	journal = {arXiv:1702.08608},
	author = {Doshi-Velez, Finale and Kim, Been},
	month = feb,
	year = {2017},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf:/Users/ukuhl/Zotero/storage/CLGYTP36/Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf:application/pdf},
}

@inproceedings{stepin_paving_2019,
	address = {Tokyo, Japan},
	title = {Paving the way towards counterfactual generation in argumentative conversational agents},
	url = {https://www.aclweb.org/anthology/W19-8405},
	doi = {10.18653/v1/W19-8405},
	abstract = {Counterfactual explanations present an effective way to interpret predictions of black-box machine learning algorithms. Whereas there is a significant body of research on counterfactual reasoning in philosophy and theoretical computer science, little attention has been paid to counterfactuals in regard to their explanatory capacity. In this paper, we review methods of argumentation theory and natural language generation that counterfactual explanation generation could benefit from most and discuss prospective directions for further research on counterfactual generation in explainable Artificial Intelligence.},
	language = {en},
	urldate = {2020-01-21},
	booktitle = {Proceedings of the 1st {Workshop} on {Interactive} {Natural} {Language} {Technology} for {Explainable} {Artificial} {Intelligence} ({NL4XAI} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Stepin, Ilia and Catala, Alejandro and Pereira-Fari{\~n}a, Martin and Alonso, Jose M.},
	year = {2019},
	pages = {20--25},
	file = {Stepin et al. - 2019 - Paving the way towards counterfactual generation i.pdf:/Users/ukuhl/Zotero/storage/PQKQZ2W7/Stepin et al. - 2019 - Paving the way towards counterfactual generation i.pdf:application/pdf},
}

@article{adadi_peeking_2018,
	title = {Peeking {Inside} the {Black}-{Box}: {A} {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Peeking {Inside} the {Black}-{Box}},
	doi = {10.1109/ACCESS.2018.2870052},
	abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
	journal = {IEEE Access},
	author = {Adadi, Amina and Berrada, Mohammed},
	year = {2018},
	keywords = {AI-based systems, artificial intelligence, Biological system modeling, black-box models, black-box nature, Conferences, explainable AI, explainable artificial intelligence, Explainable artificial intelligence, fourth industrial revolution, interpretable machine learning, Machine learning, Machine learning algorithms, Market research, Prediction algorithms, XAI},
	pages = {52138--52160},
	file = {IEEE Xplore Abstract Record:/Users/ukuhl/Zotero/storage/W67P7I9I/8466590.html:text/html;IEEE Xplore Full Text PDF:/Users/ukuhl/Zotero/storage/3E4VHELQ/Adadi und Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explaina.pdf:application/pdf},
}

@article{epstude_functional_2008,
	title = {The {Functional} {Theory} of {Counterfactual} {Thinking}},
	volume = {12},
	issn = {1088-8683},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2408534/},
	doi = {10.1177/1088868308316091},
	abstract = {Counterfactuals are thoughts about alternatives to past events, that is, thoughts of what might have been. This article provides an updated account of the functional theory of counterfactual thinking, suggesting that such thoughts are best explained in terms of their role in behavior regulation and performance improvement. The article reviews a wide range of cognitive experiments indicating that counterfactual thoughts may influence behavior by either of two routes: a content-specific pathway (which involves specific informational effects on behavioral intentions, which then influence behavior) and a content-neutral pathway (which involves indirect effects via affect, mind-sets, or motivation). The functional theory is particularly useful in organizing recent findings regarding counterfactual thinking and mental health. The article concludes by considering the connections to other theoretical conceptions, especially recent advances in goal cognition.},
	number = {2},
	journal = {Personality and social psychology review : an official journal of the Society for Personality and Social Psychology, Inc},
	author = {Epstude, Kai and Roese, Neal J.},
	month = may,
	year = {2008},
	pmid = {18453477},
	pmcid = {PMC2408534},
	pages = {168--192},
	file = {PubMed Central Full Text PDF:/Users/ukuhl/Zotero/storage/AUCJ2GQV/Epstude und Roese - 2008 - The Functional Theory of Counterfactual Thinking.pdf:application/pdf},
}

@inproceedings{offert_i_2017,
	address = {Long Beach, CA, USA},
	title = {"{I} know it when {I} see it". {Visualization} and {Intuitive} {Interpretability}},
	url = {http://arxiv.org/abs/1711.08042},
	abstract = {Most research on the interpretability of machine learning systems focuses on the development of a more rigorous notion of interpretability. I suggest that a better understanding of the deficiencies of the intuitive notion of interpretability is needed as well. I show that visualization enables but also impedes intuitive interpretability, as it presupposes two levels of technical pre-interpretation: dimensionality reduction and regularization. Furthermore, I argue that the use of positive concepts to emulate the distributed semantic structure of machine learning models introduces a significant human bias into the model. As a consequence, I suggest that, if intuitive interpretability is needed, singular representations of internal model states should be avoided.},
	booktitle = {{arXiv}:1711.08042 [stat]},
	author = {Offert, Fabian},
	month = dec,
	year = {2017},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/ANUIAQX5/Offert - 2017 - I know it when I see it. Visualization and Intui.pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/KZHL5SK7/1711.html:text/html},
}

@article{goldinger_blaming_2003,
	title = {"{Blaming} {The} {Victim}" {Under} {Memory} {Load}},
	volume = {14},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1111/1467-9280.01423},
	doi = {10.1111/1467-9280.01423},
	abstract = {When presented with negative outcomes, people often engage in counterfactual thinking, imagining various ways that events might have been different. This appears to be a spontaneous behavior, with considerable adaptive value. Nevertheless, counterfactual thinking may also engender systematic biases in various judgment tasks, such as allocating blame for a mishap, or deciding on the appropriate compensation to a victim. Thus, counterfactuals sometimes require thought suppression or discounting, potentially resource-demanding tasks. In this study, participants made mock-jury decisions about control and counterfactual versions of simple stories. The judgments of two groups of participants, differing in their respective levels of working memory capacity, were compared. In addition, all participants held memory loads during various stages of the primary task. Lower-span individuals were especially susceptible to bias associated with the counterfactual manipulation, but only when holding memory loads during judgment. The results suggest that counterfactual thoughts arise automatically, and may later require effortful, capacity-demanding suppression.},
	language = {en},
	number = {1},
	urldate = {2020-02-24},
	journal = {Psychological Science},
	author = {Goldinger, Stephen D. and Kleider, Heather M. and Azuma, Tamiko and Beike, Denise R.},
	month = jan,
	year = {2003},
	pages = {81--85},
}

@article{sanna_antecedents_1996,
	title = {Antecedents to {Spontaneous} {Counterfactual} {Thinking}: {Effects} of {Expectancy} {Violation} and {Outcome} {Valence}},
	volume = {22},
	issn = {0146-1672},
	shorttitle = {Antecedents to {Spontaneous} {Counterfactual} {Thinking}},
	url = {https://doi.org/10.1177/0146167296229005},
	doi = {10.1177/0146167296229005},
	abstract = {Three studies examined the effects of expectancy violation and outcome valence on spontaneous counterfactual thinking. In Study 1, prior expectations and outcome valence were varied orthogonally in a vignette. More counterfactuals were generated after failures and unexpected outcomes. Also, more additive than subtractive counterfatuals were found after failure, particularly unexpected failure, and more subtractive than additive counterfactuals were found after unexpected success. Evidence for the generality of these results was obtained in Study 2, in which counterfactuals were assessed after students' real-life exam performances. In Study 3, the authors further assessed nonspontaneous counterfactuals, which were shown to differ in number and structure from spontaneous counterfactuals. Discussion centers around antecedents to spontaneous counterfactual thinking and comparisons to research on spontaneous causal attributions.},
	number = {9},
	urldate = {2020-02-24},
	journal = {Personality and Social Psychology Bulletin},
	author = {Sanna, Lawrence J. and Turley, Kandi Jo},
	month = sep,
	year = {1996},
	pages = {906--919},
	file = {SAGE PDF Full Text:/Users/ukuhl/Zotero/storage/C7SWFBXA/Sanna und Turley - 1996 - Antecedents to Spontaneous Counterfactual Thinking.pdf:application/pdf},
}

@incollection{kahneman_simulation_1982,
	edition = {1},
	title = {The simulation heuristic},
	isbn = {978-0-521-28414-1 978-0-521-24064-2 978-0-511-80947-7},
	url = {https://www.cambridge.org/core/product/identifier/CBO9780511809477A026/type/book_part},
	urldate = {2020-02-25},
	booktitle = {Judgment under {Uncertainty}},
	publisher = {Cambridge University Press},
	author = {Kahneman, Daniel and Tversky, Amos},
	editor = {Kahneman, Daniel and Slovic, Paul and Tversky, Amos},
	month = apr,
	year = {1982},
	doi = {10.1017/CBO9780511809477.015},
	pages = {201--208},
	file = {Kahneman und Tversky - 1982 - The simulation heuristic.pdf:/Users/ukuhl/Zotero/storage/33KLDJ4B/Kahneman und Tversky - 1982 - The simulation heuristic.pdf:application/pdf},
}

@book{heider_psychology_1958,
	address = {New York, NY, US},
	title = {The psychology of interpersonal relations},
	publisher = {John Wiley \& Sons Ltd.},
	author = {Heider, Fritz},
	year = {1958},
}

@article{de_brigard_coming_2013,
	title = {Coming to {Grips} {With} the {Past}: {Effect} of {Repeated} {Simulation} on the {Perceived} {Plausibility} of {Episodic} {Counterfactual} {Thoughts}},
	volume = {24},
	issn = {0956-7976},
	shorttitle = {Coming to {Grips} {With} the {Past}},
	url = {https://doi.org/10.1177/0956797612468163},
	doi = {10.1177/0956797612468163},
	abstract = {When people revisit previous experiences, they often engage in episodic counterfactual thinking: mental simulations of alternative ways in which personal past events could have occurred. The present study employed a novel experimental paradigm to examine the influence of repeated simulation on the perceived plausibility of upward, downward, and neutral episodic counterfactual thoughts. Participants were asked to remember negative, positive, and neutral autobiographical memories. One week later, they self-generated upward, downward, and neutral counterfactual alternatives to those memories. The following day, they resimulated each of those counterfactuals either once or four times. The results indicate that repeated simulation of upward, downward, and neutral episodic counterfactual events decreases their perceived plausibility while increasing ratings of the ease, detail, and valence of the simulations. This finding suggests a difference between episodic counterfactual thoughts and other kinds of self-referential simulations. Possible implications of this finding for pathological and nonpathological anxiety are discussed.},
	number = {7},
	urldate = {2020-03-02},
	journal = {Psychological Science},
	author = {De Brigard, Felipe and Szpunar, Karl K. and Schacter, Daniel L.},
	month = jul,
	year = {2013},
	pages = {1329--1334},
	file = {SAGE PDF Full Text:/Users/ukuhl/Zotero/storage/K585BVLD/De Brigard et al. - 2013 - Coming to Grips With the Past Effect of Repeated .pdf:application/pdf},
}

@article{holzinger_measuring_2020,
	title = {Measuring the {Quality} of {Explanations}: {The} {System} {Causability} {Scale} ({SCS}): {Comparing} {Human} and {Machine} {Explanations}},
	volume = {34},
	issn = {0933-1875, 1610-1987},
	shorttitle = {Measuring the {Quality} of {Explanations}},
	url = {http://link.springer.com/10.1007/s13218-020-00636-z},
	doi = {10.1007/s13218-020-00636-z},
	abstract = {Recent success in Artificial Intelligence (AI) and Machine Learning (ML) allow problem solving automatically without any human intervention. Autonomous approaches can be very convenient. However, in certain domains, e.g., in the medical domain, it is necessary to enable a domain expert to understand, why an algorithm came up with a certain result. Consequently, the field of Explainable AI (xAI) rapidly gained interest worldwide in various domains, particularly in medicine. Explainable AI studies transparency and traceability of opaque AI/ML and there are already a huge variety of methods. For example with layer-wise relevance propagation relevant parts of inputs to, and representations in, a neural network which caused a result, can be highlighted. This is a first important step to ensure that end users, e.g., medical professionals, assume responsibility for decision making with AI/ML and of interest to professionals and regulators. Interactive ML adds the component of human expertise to AI/ML processes by enabling them to re-enact and retrace AI/ML results, e.g. let them check it for plausibility. This requires new human{\textendash}AI interfaces for explainable AI. In order to build effective and efficient interactive human{\textendash}AI interfaces we have to deal with the question of how to evaluate the quality of explanations given by an explainable AI system. In this paper we introduce our System Causability Scale to measure the quality of explanations. It is based on our notion of Causability (Holzinger et al. in Wiley Interdiscip Rev Data Min Knowl Discov 9(4), 2019) combined with concepts adapted from a widely-accepted usability scale.},
	language = {en},
	number = {2},
	urldate = {2020-05-25},
	journal = {KI - K{\"u}nstliche Intelligenz},
	author = {Holzinger, Andreas and Carrington, Andr{\'e} and M{\"u}ller, Heimo},
	month = jan,
	year = {2020},
	pages = {193--198},
	file = {Holzinger et al. - 2020 - Measuring the Quality of Explanations The System .pdf:/Users/ukuhl/Zotero/storage/EMUDKVCC/Holzinger et al. - 2020 - Measuring the Quality of Explanations The System .pdf:application/pdf},
}

@article{keane_if_2021,
	title = {If {Only} {We} {Had} {Better} {Counterfactual} {Explanations}: {Five} {Key} {Deficits} to {Rectify} in the {Evaluation} of {Counterfactual} {XAI} {Techniques}},
	shorttitle = {If {Only} {We} {Had} {Better} {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/2103.01035},
	abstract = {In recent years, there has been an explosion of AI research on counterfactual explanations as a solution to the problem of eXplainable AI (XAI). These explanations seem to offer technical, psychological and legal benefits over other explanation techniques. We survey 100 distinct counterfactual explanation methods reported in the literature. This survey addresses the extent to which these methods have been adequately evaluated, both psychologically and computationally, and quantifies the shortfalls occurring. For instance, only 21\% of these methods have been user tested. Five key deficits in the evaluation of these methods are detailed and a roadmap, with standardised benchmark evaluations, is proposed to resolve the issues arising; issues, that currently effectively block scientific progress in this field.},
	journal = {arXiv:2103.01035 [cs]},
	author = {Keane, Mark T. and Kenny, Eoin M. and Delaney, Eoin and Smyth, Barry},
	month = feb,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/7S2HMAX4/Keane et al. - 2021 - If Only We Had Better Counterfactual Explanations.pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/6TASU892/2103.html:text/html},
}

@inproceedings{lim_why_2009,
	address = {Boston MA USA},
	title = {\textit{{Why}} and why not explanations improve the intelligibility of context-aware intelligent systems},
	isbn = {978-1-60558-246-7},
	url = {https://dl.acm.org/doi/10.1145/1518701.1519023},
	doi = {10.1145/1518701.1519023},
	abstract = {Context-aware intelligent systems employ implicit inputs, and make decisions based on complex rules and machine learning models that are rarely clear to users. Such lack of system intelligibility can lead to loss of user trust, satisfaction and acceptance of these systems. However, automatically providing explanations about a system{\quotedblbase}s decision process can help mitigate this problem. In this paper we present results from a controlled study with over 200 participants in which the effectiveness of different types of explanations was examined. Participants were shown examples of a system{\quotedblbase}s operation along with various automatically generated explanations, and then tested on their understanding of the system. We show, for example, that explanations describing why the system behaved a certain way resulted in better understanding and stronger feelings of trust. Explanations describing why the system did not behave a certain way, resulted in lower understanding yet adequate performance. We discuss implications for the use of our findings in real-world context-aware applications.},
	language = {en},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lim, Brian Y. and Dey, Anind K. and Avrahami, Daniel},
	month = apr,
	year = {2009},
	pages = {2119--2128},
	file = {Lim et al. - 2009 - Why and why not explanations improve the in.pdf:/Users/ukuhl/Zotero/storage/3EDWH2TP/Lim et al. - 2009 - Why and why not explanations improve the in.pdf:application/pdf},
}

@article{verma_counterfactual_2020,
	title = {Counterfactual {Explanations} for {Machine} {Learning}: {A} {Review}},
	shorttitle = {Counterfactual {Explanations} for {Machine} {Learning}},
	url = {http://arxiv.org/abs/2010.10596},
	abstract = {Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.},
	journal = {arXiv:2010.10596 [cs, stat]},
	author = {Verma, Sahil and Dickerson, John and Hines, Keegan},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/N4TY4P8D/Verma et al. - 2020 - Counterfactual Explanations for Machine Learning .pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/KPZJFDGC/2010.html:text/html},
}

@article{van_der_waa_evaluating_2021,
	title = {Evaluating {XAI}: {A} comparison of rule-based and example-based explanations},
	volume = {291},
	issn = {00043702},
	shorttitle = {Evaluating {XAI}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370220301533},
	doi = {10.1016/j.artint.2020.103404},
	language = {en},
	urldate = {2021-08-10},
	journal = {Artificial Intelligence},
	author = {van der Waa, Jasper and Nieuwburg, Elisabeth and Cremers, Anita and Neerincx, Mark},
	month = feb,
	year = {2021},
	pages = {103404},
	file = {Volltext:/Users/ukuhl/Zotero/storage/XBZC9WIA/van der Waa et al. - 2021 - Evaluating XAI A comparison of rule-based and exa.pdf:application/pdf},
}

@article{guidotti_survey_2019,
	title = {A {Survey} of {Methods} for {Explaining} {Black} {Box} {Models}},
	volume = {51},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3236009},
	doi = {10.1145/3236009},
	abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
	language = {en},
	number = {5},
	journal = {ACM Computing Surveys},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	month = jan,
	year = {2019},
	pages = {1--42},
	file = {Eingereichte Version:/Users/ukuhl/Zotero/storage/I26LGY5T/Guidotti et al. - 2019 - A Survey of Methods for Explaining Black Box Model.pdf:application/pdf},
}

@article{lipton_contrastive_1990,
	title = {Contrastive {Explanation}},
	volume = {27},
	issn = {1358-2461, 1755-3555},
	url = {https://www.cambridge.org/core/product/identifier/S1358246100005130/type/journal_article},
	doi = {10.1017/S1358246100005130},
	abstract = {According to a causal model of explanation, we explain phenomena by giving their causes or, where the phenomena are themselves causal regularities, we explain them by giving a mechanism linking cause and effect. If we explain why smoking causes cancer, we do not give the cause of this causal connection, but we do give the causal mechanism that makes it. The claim that to explain is to give a cause is not only natural and plausible, but it also avoids many of the objections to other accounts of explanation, such as the views that to explain is to give a reason to believe the phenomenon occurred, to somehow make the phenomenon familiar, or to give a Deductive-Nomological argument. Unlike the reason for belief account, a causal model makes a clear distinction between understanding why a phenomenon occurs and merely knowing that it does, and the model does so in a way that makes understanding unmysterious and objective. Understanding is not some sort of super-knowledge, but simply more knowledge: knowledge of the phenomenon and knowledge of its causal history. A causal model makes it clear how something can explain without itself being explained, and so avoids the regress of whys, since we can know a phenomenon's cause without knowing the cause of the cause. It also accounts for legitimate self-evidencing explanations, explanations where the phenomenon is an essential part of the evidence that the explanation is correct, so the explanation can not supply a non-circular reason for believing the phenomenon occurred. There is no barrier to knowing a cause through its effects and also knowing that it is their cause. The speed of recession of a star explains its observed red-shift, even though the shift is an essential part of the evidence for its speed of recession. The model also avoids the most serious objection to the familiarity view, which is that some phenomena are familiar yet not understood, since a phenomenon can be perfectly familiar, such as the blueness of the sky or the fact that the same side of the moon always faces the earth, even if we do not know its cause. Finally, a causal model avoids many of the objections to the Deductive-Nomological model. Ordinary explanations do not have to meet the requirements of the Deductive-Nomological model, because one does not need to give a law to give a cause, and one does not need to know a law to have good reason to believe that a cause is a cause. As for the notorious over-permissiveness of the Deductive-Nomological model, the reason recession explains red-shift but not conversely, is simply that causes explain effects but not conversely, and the reason a conjunction of laws does not explain its conjuncts is that conjunctions do not cause their conjuncts.},
	language = {en},
	urldate = {2021-08-13},
	journal = {Royal Institute of Philosophy Supplement},
	author = {Lipton, Peter},
	month = mar,
	year = {1990},
	pages = {247--266},
	file = {Lipton - 1990 - Contrastive Explanation.pdf:/Users/ukuhl/Zotero/storage/Z963NVIX/Lipton - 1990 - Contrastive Explanation.pdf:application/pdf},
}

@incollection{lombrozo_explanation_2012,
	title = {Explanation and {Abductive} {Inference}},
	url = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780199734689.001.0001/oxfordhb-9780199734689-e-14},
	urldate = {2021-08-13},
	booktitle = {The {Oxford} {Handbook} of {Thinking} and {Reasoning}},
	publisher = {Oxford University Press},
	author = {Lombrozo, Tania},
	editor = {Holyoak, Keith J. and Morrison, Robert G.},
	month = mar,
	year = {2012},
	doi = {10.1093/oxfordhb/9780199734689.013.0014},
	pages = {260--276},
	file = {Lombrozo - 2012 - Explanation and Abductive Inference.pdf:/Users/ukuhl/Zotero/storage/UBDU978U/Lombrozo - 2012 - Explanation and Abductive Inference.pdf:application/pdf},
}

@article{hilton_knowledge-based_1986,
	title = {Knowledge-based causal attribution: {The} abnormal conditions focus model.},
	volume = {93},
	issn = {1939-1471, 0033-295X},
	shorttitle = {Knowledge-based causal attribution},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.93.1.75},
	doi = {10.1037/0033-295X.93.1.75},
	language = {en},
	number = {1},
	urldate = {2021-08-13},
	journal = {Psychological Review},
	author = {Hilton, Denis J. and Slugoski, Ben R.},
	year = {1986},
	pages = {75--88},
	file = {Hilton und Slugoski - 1986 - Knowledge-based causal attribution The abnormal c.pdf:/Users/ukuhl/Zotero/storage/FXT2TYJS/Hilton und Slugoski - 1986 - Knowledge-based causal attribution The abnormal c.pdf:application/pdf},
}

@article{roese_counterfactual_1997,
	title = {Counterfactual thinking.},
	volume = {121},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.121.1.133},
	doi = {10.1037/0033-2909.121.1.133},
	language = {en},
	number = {1},
	urldate = {2021-08-13},
	journal = {Psychological Bulletin},
	author = {Roese, Neal J.},
	year = {1997},
	pages = {133--148},
	file = {Roese - 1997 - Counterfactual thinking..pdf:/Users/ukuhl/Zotero/storage/XHVGPZ2R/Roese - 1997 - Counterfactual thinking..pdf:application/pdf},
}

@incollection{roese_functional_2017,
	title = {The {Functional} {Theory} of {Counterfactual} {Thinking}: {New} {Evidence}, {New} {Challenges}, {New} {Insights}},
	volume = {56},
	isbn = {978-0-12-812120-7},
	shorttitle = {The {Functional} {Theory} of {Counterfactual} {Thinking}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0065260117300187},
	language = {en},
	urldate = {2021-08-16},
	booktitle = {Advances in {Experimental} {Social} {Psychology}},
	publisher = {Elsevier},
	author = {Roese, Neal J. and Epstude, Kai},
	year = {2017},
	doi = {10.1016/bs.aesp.2017.02.001},
	pages = {1--79},
	file = {Volltext:/Users/ukuhl/Zotero/storage/FXRVS769/Roese und Epstude - 2017 - The Functional Theory of Counterfactual Thinking .pdf:application/pdf},
}

@article{markman_reflection_2003,
	title = {A {Reflection} and {Evaluation} {Model} of {Comparative} {Thinking}},
	volume = {7},
	issn = {1088-8683, 1532-7957},
	url = {http://journals.sagepub.com/doi/10.1207/S15327957PSPR0703_04},
	doi = {10.1207/S15327957PSPR0703_04},
	abstract = {This article reviews research on counterfactual, social, and temporal comparisons and proposes a Reflection and Evaluation Model (REM) as an organizing framework. At the heart of the model is the assertion that 2 psychologically distinct modes of mental simulation operate during comparative thinking: reflection, an experiential ({\textquotedblleft}as if{\textquotedblright}) mode of thinking characterized by vividly simulating that information about the comparison standard is true of, or part of, the self; and evaluation, an evaluative mode of thinking characterized by the use of information about the standard as a reference point against which to evaluate one's present standing. Reflection occurs when information about the standard is included in one's self-construal, and evaluation occurs when such information is excluded. The result of reflection is that standard-consistent cognitions about the self become highly accessible, thereby yielding affective assimilation; whereas the result of evaluation is that comparison information is used as a standard against which one's present standing is evaluated, thereby yielding affective contrast. The resulting affect leads to either an increase or decrease in behavioral persistence as a function of the type of task with which one is engaged, and a combination of comparison-derived causal inferences and regulatory focus strategies direct one toward adopting specific future action plans.},
	language = {en},
	number = {3},
	urldate = {2021-08-16},
	journal = {Personality and Social Psychology Review},
	author = {Markman, Keith D. and McMullen, Matthew N.},
	month = aug,
	year = {2003},
	pages = {244--267},
}

@inproceedings{dandl_multi-objective_2020,
	address = {Cham},
	title = {Multi-{Objective} {Counterfactual} {Explanations}},
	volume = {12269},
	isbn = {978-3-030-58111-4 978-3-030-58112-1},
	url = {http://link.springer.com/10.1007/978-3-030-58112-1_31},
	doi = {10.1007/978-3-030-58112-1_31},
	language = {en},
	urldate = {2021-08-16},
	booktitle = {International {Conference} on {Parallel} {Problem} {Solving} from {Nature}},
	publisher = {Springer International Publishing},
	author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
	year = {2020},
	pages = {448--469},
	file = {Volltext:/Users/ukuhl/Zotero/storage/4NPKM7WQ/Dandl et al. - 2020 - Multi-Objective Counterfactual Explanations.pdf:application/pdf},
}

@article{guidotti_local_2018,
	title = {Local {Rule}-{Based} {Explanations} of {Black} {Box} {Decision} {Systems}},
	url = {http://arxiv.org/abs/1805.10820},
	abstract = {The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of machine learning components in socially sensitive and safety-critical contexts. \%Therefore, we need explanations that reveals the reasons why a predictor takes a certain decision. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.},
	urldate = {2021-08-16},
	journal = {arXiv:1805.10820 [cs]},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco and Giannotti, Fosca},
	month = may,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/WCLUSE53/Guidotti et al. - 2018 - Local Rule-Based Explanations of Black Box Decisio.pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/TALKXILN/1805.html:text/html},
}

@incollection{artelt_convex_2020,
	address = {Cham},
	title = {Convex {Density} {Constraints} for {Computing} {Plausible} {Counterfactual} {Explanations}},
	volume = {12396},
	isbn = {978-3-030-61608-3},
	url = {https://link.springer.com/10.1007/978-3-030-61609-0_28},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} {\textendash} {ICANN} 2020},
	publisher = {Springer International Publishing},
	author = {Artelt, Andr{\'e} and Hammer, Barbara},
	editor = {Farka{\v s}, Igor and Masulli, Paolo and Wermter, Stefan},
	year = {2020},
	doi = {10.1007/978-3-030-61609-0_28},
	pages = {353--365},
	file = {Eingereichte Version:/Users/ukuhl/Zotero/storage/5JNBEAJS/Artelt und Hammer - 2020 - Convex Density Constraints for Computing Plausible.pdf:application/pdf},
}

@article{chi_icap_2014,
	title = {The {ICAP} {Framework}: {Linking} {Cognitive} {Engagement} to {Active} {Learning} {Outcomes}},
	volume = {49},
	issn = {0046-1520, 1532-6985},
	shorttitle = {The {ICAP} {Framework}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00461520.2014.965823},
	doi = {10.1080/00461520.2014.965823},
	language = {en},
	number = {4},
	urldate = {2021-08-25},
	journal = {Educational Psychologist},
	author = {Chi, Michelene T. H. and Wylie, Ruth},
	month = oct,
	year = {2014},
	pages = {219--243},
}

@article{detry_analyzing_2016,
	title = {Analyzing {Repeated} {Measurements} {Using} {Mixed} {Models}},
	volume = {315},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2015.19394},
	doi = {10.1001/jama.2015.19394},
	language = {en},
	number = {4},
	urldate = {2021-10-14},
	journal = {JAMA},
	author = {Detry, Michelle A. and Ma, Yan},
	month = jan,
	year = {2016},
	pages = {407},
	file = {Detry und Ma - 2016 - Analyzing Repeated Measurements Using Mixed Models.pdf:/Users/ukuhl/Zotero/storage/GEMAWZUT/Detry und Ma - 2016 - Analyzing Repeated Measurements Using Mixed Models.pdf:application/pdf},
}

@book{r_core_team_r_2021,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org/},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2021},
}

@article{bates_fitting_2015,
	title = {Fitting {Linear} {Mixed}-{Effects} {Models} {Using} \textbf{lme4}},
	volume = {67},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v67/i01/},
	doi = {10.18637/jss.v067.i01},
	language = {en},
	number = {1},
	urldate = {2021-11-16},
	journal = {Journal of Statistical Software},
	author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
	year = {2015},
	file = {Volltext:/Users/ukuhl/Zotero/storage/JZH5UMYC/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf:application/pdf},
}

@article{ben-shachar_effectsize_2020,
	title = {effectsize: {Estimation} of {Effect} {Size} {Indices} and {Standardized} {Parameters}},
	volume = {5},
	issn = {2475-9066},
	shorttitle = {effectsize},
	url = {https://joss.theoj.org/papers/10.21105/joss.02815},
	doi = {10.21105/joss.02815},
	number = {56},
	urldate = {2021-11-16},
	journal = {Journal of Open Source Software},
	author = {Ben-Shachar, Mattan and L{\"u}decke, Daniel and Makowski, Dominique},
	month = dec,
	year = {2020},
	pages = {2815},
	file = {Volltext:/Users/ukuhl/Zotero/storage/7QM6H6R7/Ben-Shachar et al. - 2020 - effectsize Estimation of Effect Size Indices and .pdf:application/pdf},
}

@article{bansal_updates_2019,
	title = {Updates in {Human}-{AI} {Teams}: {Understanding} and {Addressing} the {Performance}/{Compatibility} {Tradeoff}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Updates in {Human}-{AI} {Teams}},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/4087},
	doi = {10.1609/aaai.v33i01.33012429},
	abstract = {AI systems are being deployed to support human decision making in high-stakes domains such as healthcare and criminal justice. In many cases, the human and AI form a team, in which the human makes decisions after reviewing the AI{\textquoteright}s inferences. A successful partnership requires that the human develops insights into the performance of the AI system, including its failures. We study the influence of updates to an AI system in this setting. While updates can increase the AI{\textquoteright}s predictive performance, they may also lead to behavioral changes that are at odds with the user{\textquoteright}s prior experiences and confidence in the AI{\textquoteright}s inferences. We show that updates that increase AI performance may actually hurt team performance. We introduce the notion of the compatibility of an AI update with prior user experience and present methods for studying the role of compatibility in human-AI teams. Empirical results on three high-stakes classification tasks show that current machine learning algorithms do not produce compatible updates. We propose a re-training objective to improve the compatibility of an update by penalizing new errors. The objective offers full leverage of the performance/compatibility tradeoff across different datasets, enabling more compatible yet accurate updates.},
	urldate = {2021-11-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Bansal, Gagan and Nushi, Besmira and Kamar, Ece and Weld, Daniel S. and Lasecki, Walter S. and Horvitz, Eric},
	month = jul,
	year = {2019},
	pages = {2429--2437},
	file = {Volltext:/Users/ukuhl/Zotero/storage/GDKIPZAA/Bansal et al. - 2019 - Updates in Human-AI Teams Understanding and Addre.pdf:application/pdf},
}

@article{muth_alternative_2016,
	title = {Alternative {Models} for {Small} {Samples} in {Psychological} {Research}: {Applying} {Linear} {Mixed} {Effects} {Models} and {Generalized} {Estimating} {Equations} to {Repeated} {Measures} {Data}},
	volume = {76},
	issn = {0013-1644, 1552-3888},
	shorttitle = {Alternative {Models} for {Small} {Samples} in {Psychological} {Research}},
	url = {http://journals.sagepub.com/doi/10.1177/0013164415580432},
	doi = {10.1177/0013164415580432},
	abstract = {Unavoidable sample size issues beset psychological research that involves scarce populations or costly laboratory procedures. When incorporating longitudinal designs these samples are further reduced by traditional modeling techniques, which perform listwise deletion for any instance of missing data. Moreover, these techniques are limited in their capacity to accommodate alternative correlation structures that are common in repeated measures studies. Researchers require sound quantitative methods to work with limited but valuable measures without degrading their data sets. This article provides a brief tutorial and exploration of two alternative longitudinal modeling techniques, linear mixed effects models and generalized estimating equations, as applied to a repeated measures study ( n = 12) of pairmate attachment and social stress in primates. Both techniques provide comparable results, but each model offers unique information that can be helpful when deciding the right analytic tool.},
	language = {en},
	number = {1},
	urldate = {2021-11-18},
	journal = {Educational and Psychological Measurement},
	author = {Muth, Chelsea and Bales, Karen L. and Hinde, Katie and Maninger, Nicole and Mendoza, Sally P. and Ferrer, Emilio},
	month = feb,
	year = {2016},
	pages = {64--87},
	file = {Volltext:/Users/ukuhl/Zotero/storage/6YL7S6G2/Muth et al. - 2016 - Alternative Models for Small Samples in Psychologi.pdf:application/pdf},
}

@article{artelt_evaluating_2021,
	title = {Evaluating {Robustness} of {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/2103.02354},
	abstract = {Transparency is a fundamental requirement for decision making systems when these should be deployed in the real world. It is usually achieved by providing explanations of the system's behavior. A prominent and intuitive type of explanations are counterfactual explanations. Counterfactual explanations explain a behavior to the user by proposing actions {\textendash} as changes to the input {\textendash} that would cause a different (specified) behavior of the system. However, such explanation methods can be unstable with respect to small changes to the input {\textendash} i.e. even a small change in the input can lead to huge or arbitrary changes in the output and of the explanation. This could be problematic for counterfactual explanations, as two similar individuals might get very different explanations. Even worse, if the recommended actions differ considerably in their complexity, one would consider such unstable (counterfactual) explanations as individually unfair. In this work, we formally and empirically study the robustness of counterfactual explanations in general, as well as under different models and different kinds of perturbations. Furthermore, we propose that plausible counterfactual explanations can be used instead of closest counterfactual explanations to improve the robustness and consequently the individual fairness of counterfactual explanations.},
	urldate = {2021-11-30},
	journal = {arXiv:2103.02354 [cs]},
	author = {Artelt, Andr{\'e} and Vaquet, Valerie and Velioglu, Riza and Hinder, Fabian and Brinkrolf, Johannes and Schilling, Malte and Hammer, Barbara},
	month = jul,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/52R2A4AR/Artelt et al. - 2021 - Evaluating Robustness of Counterfactual Explanatio.pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/RRE86KCW/2103.html:text/html},
}

@article{logan_shapes_1992,
	title = {Shapes of {Reaction}-{Time} {Distributions} and {Shapes} of {Learning} {Curves}: {A} {Test} of the {Instance} {Theory} of {Automaticity}},
	volume = {18},
	language = {en},
	number = {5},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Logan, Gordon D},
	year = {1992},
	pages = {883--914},
	file = {Logan - Shapes of Reaction-Time Distributions and Shapes o.pdf:/Users/ukuhl/Zotero/storage/GI8AD66Y/Logan - Shapes of Reaction-Time Distributions and Shapes o.pdf:application/pdf},
}

@article{artelt_efficient_2022,
	title = {Efficient computation of counterfactual explanations and counterfactual metrics of prototype-based classifiers},
	volume = {470},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221011024},
	doi = {10.1016/j.neucom.2021.04.129},
	language = {en},
	urldate = {2021-11-30},
	journal = {Neurocomputing},
	author = {Artelt, Andr{\'e} and Hammer, Barbara},
	month = jan,
	year = {2022},
	pages = {304--317},
}

@article{kumle_estimating_2021,
	title = {Estimating power in (generalized) linear mixed models: {An} open introduction and tutorial in {R}},
	volume = {53},
	issn = {1554-3528},
	shorttitle = {Estimating power in (generalized) linear mixed models},
	url = {https://link.springer.com/10.3758/s13428-021-01546-0},
	doi = {10.3758/s13428-021-01546-0},
	abstract = {Abstract Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to analytic power solutions are simulation-based power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
	language = {en},
	number = {6},
	urldate = {2021-11-30},
	journal = {Behavior Research Methods},
	author = {Kumle, Leah and V{\~o}, Melissa L.-H. and Draschkow, Dejan},
	month = may,
	year = {2021},
	pages = {2528--2543},
	file = {Volltext:/Users/ukuhl/Zotero/storage/I7A576AN/Kumle et al. - 2021 - Estimating power in (generalized) linear mixed mod.pdf:application/pdf},
}

@article{miller_explanation_2019,
	title = {Explanation in artificial intelligence: {Insights} from the social sciences},
	volume = {267},
	issn = {00043702},
	shorttitle = {Explanation in artificial intelligence},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370218305988},
	doi = {10.1016/j.artint.2018.07.007},
	language = {en},
	urldate = {2021-11-30},
	journal = {Artificial Intelligence},
	author = {Miller, Tim},
	month = feb,
	year = {2019},
	pages = {1--38},
	file = {Eingereichte Version:/Users/ukuhl/Zotero/storage/8H3HEHQ5/Miller - 2019 - Explanation in artificial intelligence Insights f.pdf:application/pdf},
}

@article{gleaves_role_2020,
	title = {The {Role} of {Individual} {User} {Differences} in {Interpretable} and {Explainable} {Machine} {Learning} {Systems}},
	url = {http://arxiv.org/abs/2009.06675},
	abstract = {There is increased interest in assisting non-expert audiences to effectively interact with machine learning (ML) tools and understand the complex output such systems produce. Here, we describe user experiments designed to study how individual skills and personality traits predict interpretability, explainability, and knowledge discovery from ML generated model output. Our work relies on Fuzzy Trace Theory, a leading theory of how humans process numerical stimuli, to examine how different end users will interpret the output they receive while interacting with the ML system. While our sample was small, we found that interpretability {\textendash} being able to make sense of system output {\textendash} and explainability {\textendash} understanding how that output was generated {\textendash} were distinct aspects of user experience. Additionally, subjects were more able to interpret model output if they possessed individual traits that promote metacognitive monitoring and editing, associated with more detailed, verbatim, processing of ML output. Finally, subjects who are more familiar with ML systems felt better supported by them and more able to discover new patterns in data; however, this did not necessarily translate to meaningful insights. Our work motivates the design of systems that explicitly take users' mental representations into account during the design process to more effectively support end user requirements.},
	urldate = {2021-11-30},
	journal = {arXiv:2009.06675 [cs]},
	author = {Gleaves, Lydia P. and Schwartz, Reva and Broniatowski, David A.},
	month = sep,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/9KAPHRW2/Gleaves et al. - 2020 - The Role of Individual User Differences in Interpr.pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/U4W37DQU/2009.html:text/html},
}

@article{schleich_geco_2021,
	title = {{GeCo}: {Quality} {Counterfactual} {Explanations} in {Real} {Time}},
	shorttitle = {{GeCo}},
	url = {http://arxiv.org/abs/2101.01292},
	abstract = {Machine learning is increasingly applied in high-stakes decision making that directly affect people's lives, and this leads to an increased demand for systems to explain their decisions. Explanations often take the form of counterfactuals, which consists of conveying to the end user what she/he needs to change in order to improve the outcome. Computing counterfactual explanations is challenging, because of the inherent tension between a rich semantics of the domain, and the need for real time response. In this paper we present GeCo, the first system that can compute plausible and feasible counterfactual explanations in real time. At its core, GeCo relies on a genetic algorithm, which is customized to favor searching counterfactual explanations with the smallest number of changes. To achieve real-time performance, we introduce two novel optimizations: \${\textbackslash}textbackslashDelta\$-representation of candidate counterfactuals, and partial evaluation of the classifier. We compare empirically GeCo against five other systems described in the literature, and show that it is the only system that can achieve both high quality explanations and real time answers.},
	urldate = {2021-11-30},
	journal = {arXiv:2101.01292 [cs]},
	author = {Schleich, Maximilian and Geng, Zixuan and Zhang, Yihong and Suciu, Dan},
	month = may,
	year = {2021},
	keywords = {Computer Science - Databases, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/44E4TKRS/Schleich et al. - 2021 - GeCo Quality Counterfactual Explanations in Real .pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/J9SAN7F5/2101.html:text/html},
}

@article{johnson-laird_conditionals_2002,
	title = {Conditionals: {A} theory of meaning, pragmatics, and inference.},
	volume = {109},
	issn = {1939-1471, 0033-295X},
	shorttitle = {Conditionals},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.109.4.646},
	doi = {10.1037/0033-295X.109.4.646},
	language = {en},
	number = {4},
	urldate = {2021-12-01},
	journal = {Psychological Review},
	author = {Johnson-Laird, P. N. and Byrne, Ruth M. J.},
	year = {2002},
	pages = {646--678},
	file = {Volltext:/Users/ukuhl/Zotero/storage/B63B39CT/Johnson-Laird und Byrne - 2002 - Conditionals A theory of meaning, pragmatics, and.pdf:application/pdf},
}

@article{byrne_mental_2002,
	title = {Mental models and counterfactual thoughts about what might have been},
	volume = {6},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661302019745},
	doi = {10.1016/S1364-6613(02)01974-5},
	language = {en},
	number = {10},
	urldate = {2021-12-01},
	journal = {Trends in Cognitive Sciences},
	author = {Byrne, Ruth M.J.},
	month = oct,
	year = {2002},
	pages = {426--431},
	file = {Byrne - 2002 - Mental models and counterfactual thoughts about wh.pdf:/Users/ukuhl/Zotero/storage/8H3S89H7/Byrne - 2002 - Mental models and counterfactual thoughts about wh.pdf:application/pdf},
}

@article{byrne_precis_2007,
	title = {Pr{\'e}cis of \textit{{The}} {Rational} {Imagination}: {How} {People} {Create} {Alternatives} to {Reality}},
	volume = {30},
	issn = {0140-525X, 1469-1825},
	shorttitle = {Pr{\'e}cis of {\textbackslash}textlessi{\textbackslash}{textgreaterThe} {Rational} {Imagination}},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X07002579/type/journal_article},
	doi = {10.1017/S0140525X07002579},
	abstract = {Abstract The human imagination remains one of the last uncharted terrains of the mind. People often imagine how events might have turned out {\textquotedblleft}if only{\textquotedblright} something had been different. The {\textquotedblleft}fault lines{\textquotedblright} of reality, those aspects more readily changed, indicate that counterfactual thoughts are guided by the same principles as rational thoughts. In the past, rationality and imagination have been viewed as opposites. But research has shown that rational thought is more imaginative than cognitive scientists had supposed. In The Rational Imagination, I argue that imaginative thought is more rational than scientists have imagined. People exhibit remarkable similarities in the sorts of things they change in their mental representation of reality when they imagine how the facts could have turned out differently. For example, they tend to imagine alternatives to actions rather than inactions, events within their control rather than those beyond their control, and socially unacceptable events rather than acceptable ones. Their thoughts about how an event might have turned out differently lead them to judge that a strong causal relation exists between an antecedent event and the outcome, and their thoughts about how an event might have turned out the same lead them to judge that a weaker causal relation exists. In a simple temporal sequence, people tend to imagine alternatives to the most recent event. The central claim in the book is that counterfactual thoughts are organised along the same principles as rational thought. The idea that the counterfactual imagination is rational depends on three steps: (1) humans are capable of rational thought; (2) they make inferences by thinking about possibilities; and (3) their counterfactual thoughts rely on thinking about possibilities, just as rational thoughts do. The sorts of possibilities that people envisage explain the mutability of certain aspects of mental representations and the immutability of other aspects.},
	language = {en},
	number = {5-6},
	urldate = {2021-12-01},
	journal = {Behavioral and Brain Sciences},
	author = {Byrne, Ruth M. J.},
	month = dec,
	year = {2007},
	pages = {439--453},
}

@incollection{walsh_mental_2005,
	address = {London},
	series = {Routledge {Research} {International} {Series} in {Social} {Psychology}},
	title = {The mental representation of what might have been},
	booktitle = {The psychology of counterfactual thinking},
	publisher = {Routledge},
	author = {Walsh, C.R. and Byrne, Ruth M.J.},
	editor = {Mandel, D.R. and Hilton, Denis J. and Catellani, P.},
	year = {2005},
	pages = {61--73},
}

@incollection{lewis_counterfactuals_1973,
	address = {Dordrecht},
	title = {Counterfactuals and {Comparative} {Possibility}},
	isbn = {978-90-277-1220-2 978-94-009-9117-0},
	url = {http://link.springer.com/10.1007/978-94-009-9117-0_3},
	urldate = {2021-12-01},
	booktitle = {{IFS}},
	publisher = {Springer Netherlands},
	author = {Lewis, David},
	editor = {Harper, William L. and Stalnaker, Robert and Pearce, Glenn},
	year = {1973},
	doi = {10.1007/978-94-009-9117-0_3},
	pages = {57--85},
	file = {Lewis - 1973 - Counterfactuals and Comparative Possibility.pdf:/Users/ukuhl/Zotero/storage/M3VPHK8U/Lewis - 1973 - Counterfactuals and Comparative Possibility.pdf:application/pdf},
}

@article{stanley_counterfactual_2017,
	title = {Counterfactual {Plausibility} and {Comparative} {Similarity}},
	volume = {41},
	issn = {03640213},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.12451},
	doi = {10.1111/cogs.12451},
	language = {en},
	urldate = {2021-12-01},
	journal = {Cognitive Science},
	author = {Stanley, Matthew L. and Stewart, Gregory W. and Brigard, Felipe De},
	month = may,
	year = {2017},
	pages = {1216--1228},
	file = {Volltext:/Users/ukuhl/Zotero/storage/TJBSXIPX/Stanley et al. - 2017 - Counterfactual Plausibility and Comparative Simila.pdf:application/pdf},
}

@article{kulakova_processing_2013,
	title = {Processing counterfactual and hypothetical conditionals: {An} {fMRI} investigation},
	volume = {72},
	issn = {10538119},
	shorttitle = {Processing counterfactual and hypothetical conditionals},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811913001079},
	doi = {10.1016/j.neuroimage.2013.01.060},
	language = {en},
	urldate = {2021-12-01},
	journal = {NeuroImage},
	author = {Kulakova, Eugenia and Aichhorn, Markus and Schurz, Matthias and Kronbichler, Martin and Perner, Josef},
	month = may,
	year = {2013},
	pages = {265--271},
	file = {Volltext:/Users/ukuhl/Zotero/storage/WBTSJWR2/Kulakova et al. - 2013 - Processing counterfactual and hypothetical conditi.pdf:application/pdf},
}

@article{de_brigard_perceived_2021,
	title = {Perceived similarity of imagined possible worlds affects judgments of counterfactual plausibility},
	volume = {209},
	issn = {00100277},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027720303930},
	doi = {10.1016/j.cognition.2020.104574},
	language = {en},
	urldate = {2021-12-01},
	journal = {Cognition},
	author = {De Brigard, Felipe and Henne, Paul and Stanley, Matthew L.},
	month = apr,
	year = {2021},
	pages = {104574},
}

@article{connell_model_2006,
	title = {A {Model} of {Plausibility}},
	volume = {30},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1207/s15516709cog0000_53},
	doi = {10.1207/s15516709cog0000_53},
	language = {en},
	number = {1},
	urldate = {2021-12-01},
	journal = {Cognitive Science},
	author = {Connell, Louise and Keane, Mark T.},
	month = jan,
	year = {2006},
	pages = {95--120},
	file = {Connell und Keane - 2006 - A Model of Plausibility.pdf:/Users/ukuhl/Zotero/storage/NUB9C5BE/Connell und Keane - 2006 - A Model of Plausibility.pdf:application/pdf},
}

@article{byrne_temporality_2000,
	title = {The temporality effect in counterfactual thinking about what might have been},
	volume = {28},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/BF03213805},
	doi = {10.3758/BF03213805},
	language = {en},
	number = {2},
	urldate = {2021-12-02},
	journal = {Memory \& Cognition},
	author = {Byrne, Ruth M. J. and Segura, Susana and Culhane, Ronan and Tasso, Alessandra and Berrocal, Pablo},
	month = mar,
	year = {2000},
	pages = {264--281},
	file = {Volltext:/Users/ukuhl/Zotero/storage/2BVSZ82R/Byrne et al. - 2000 - The temporality effect in counterfactual thinking .pdf:application/pdf},
}

@article{miller_temporal_1990,
	title = {Temporal order and the perceived mutability of events: {Implications} for blame assignment.},
	volume = {59},
	issn = {1939-1315, 0022-3514},
	shorttitle = {Temporal order and the perceived mutability of events},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.59.6.1111},
	doi = {10.1037/0022-3514.59.6.1111},
	language = {en},
	number = {6},
	urldate = {2021-12-02},
	journal = {Journal of Personality and Social Psychology},
	author = {Miller, Dale T. and Gunasegaram, Saku},
	year = {1990},
	pages = {1111--1118},
}

@article{dixon_if_2011,
	title = {{\textquotedblleft}{If} only{\textquotedblright} counterfactual thoughts about exceptional actions},
	volume = {39},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/s13421-011-0101-4},
	doi = {10.3758/s13421-011-0101-4},
	language = {en},
	number = {7},
	urldate = {2021-12-02},
	journal = {Memory \& Cognition},
	author = {Dixon, James E. and Byrne, Ruth M. J.},
	month = oct,
	year = {2011},
	pages = {1317--1331},
}

@article{girotto_event_1991,
	title = {Event controllability in counterfactual thinking},
	volume = {78},
	issn = {00016918},
	url = {https://linkinghub.elsevier.com/retrieve/pii/000169189190007M},
	doi = {10.1016/0001-6918(91)90007-M},
	language = {en},
	number = {1-3},
	urldate = {2021-12-02},
	journal = {Acta Psychologica},
	author = {Girotto, Vittorio and Legrenzi, Paolo and Rizzo, Antonio},
	month = dec,
	year = {1991},
	pages = {111--133},
}

@article{pezdek_is_2006,
	title = {Is knowing believing? {The} role of event plausibility and background knowledge in planting false beliefs about the personal past},
	volume = {34},
	issn = {0090-502X, 1532-5946},
	shorttitle = {Is knowing believing?},
	url = {http://link.springer.com/10.3758/BF03195925},
	doi = {10.3758/BF03195925},
	language = {en},
	number = {8},
	urldate = {2021-12-02},
	journal = {Memory \& Cognition},
	author = {Pezdek, Kathy and Blandon-Gitlin, Iris and Lam, Shirley and Hart, Rhiannon Ellis and Schooler, Jonathan W.},
	month = dec,
	year = {2006},
	pages = {1628--1635},
}

@article{de_brigard_remembering_2013,
	title = {Remembering what could have happened: {Neural} correlates of episodic counterfactual thinking},
	volume = {51},
	issn = {00283932},
	shorttitle = {Remembering what could have happened},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0028393213000298},
	doi = {10.1016/j.neuropsychologia.2013.01.015},
	language = {en},
	number = {12},
	urldate = {2021-12-02},
	journal = {Neuropsychologia},
	author = {De Brigard, F. and Addis, D.R. and Ford, J.H. and Schacter, D.L. and Giovanello, K.S.},
	month = oct,
	year = {2013},
	pages = {2401--2414},
	file = {Akzeptierte Version:/Users/ukuhl/Zotero/storage/PRZSCFDZ/De Brigard et al. - 2013 - Remembering what could have happened Neural corre.pdf:application/pdf},
}

@article{medvec_when_1997,
	title = {When doing better means feeling worse: {The} effects of categorical cutoff points on counterfactual thinking and satisfaction.},
	volume = {72},
	issn = {1939-1315, 0022-3514},
	shorttitle = {When doing better means feeling worse},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.72.6.1284},
	doi = {10.1037/0022-3514.72.6.1284},
	language = {en},
	number = {6},
	urldate = {2021-12-03},
	journal = {Journal of Personality and Social Psychology},
	author = {Medvec, Victoria Husted and Savitsky, Kenneth},
	year = {1997},
	pages = {1284--1296},
}

@article{de_oliveira_framework_2021,
	title = {A {Framework} and {Benchmarking} {Study} for {Counterfactual} {Generating} {Methods} on {Tabular} {Data}},
	volume = {11},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/16/7274},
	doi = {10.3390/app11167274},
	abstract = {Counterfactual explanations are viewed as an effective way to explain machine learning predictions. This interest is reflected by a relatively young literature with already dozens of algorithms aiming to generate such explanations. These algorithms are focused on finding how features can be modified to change the output classification. However, this rather general objective can be achieved in different ways, which brings about the need for a methodology to test and benchmark these algorithms. The contributions of this work are manifold: First, a large benchmarking study of 10 algorithmic approaches on 22 tabular datasets is performed, using nine relevant evaluation metrics; second, the introduction of a novel, first of its kind, framework to test counterfactual generation algorithms; third, a set of objective metrics to evaluate and compare counterfactual results; and, finally, insight from the benchmarking results that indicate which approaches obtain the best performance on what type of dataset. This benchmarking study and framework can help practitioners in determining which technique and building blocks most suit their context, and can help researchers in the design and evaluation of current and future counterfactual generation algorithms. Our findings show that, overall, there{\textquoteright}s no single best algorithm to generate counterfactual explanations as the performance highly depends on properties related to the dataset, model, score, and factual point specificities.},
	language = {en},
	number = {16},
	urldate = {2022-01-05},
	journal = {Applied Sciences},
	author = {de Oliveira, Raphael Mazzine Barbosa and Martens, David},
	month = aug,
	year = {2021},
	pages = {7274},
}

@inproceedings{white_measurable_2020,
	address = {Santiago de Compostela, Spain},
	title = {Measurable {Counterfactual} {Local} {Explanations} for {Any} {Classifier}},
	abstract = {We propose a novel method for explaining the predictions of any classifier. In our approach, local explanations are expected to explain both the outcome of a prediction and how that prediction would change if {\textquoteright}things had been different{\textquoteright}. Furthermore, we argue that satisfactory explanations cannot be dissociated from a notion and measure of fidelity, as advocated in the early days of neural networks{\textquoteright} knowledge extraction. We introduce a definition of fidelity to the underlying classifier for local explanation models which is based on distances to a target decision boundary. A system called CLEAR: Counterfactual Local Explanations via Regression, is introduced and evaluated. CLEAR generates b-counterfactual explanations that state minimum changes necessary to flip a prediction{\textquoteright}s classification. CLEAR then builds local regression models, using the b-counterfactuals to measure and improve the fidelity of its regressions. By contrast, the popular LIME method [17], which also uses regression to generate local explanations, neither measures its own fidelity nor generates counterfactuals. CLEAR{\textquoteright}s regressions are found to have significantly higher fidelity than LIME{\textquoteright}s, averaging over 40\% higher in this paper{\textquoteright}s five case studies.},
	language = {en},
	author = {White, Adam and d{\textquoteright}Avila Garcez, Artur},
	year = {2020},
	pages = {7},
}

@book{breiman_classification_1984,
	edition = {1},
	title = {Classification {And} {Regression} {Trees}},
	isbn = {978-1-315-13947-0},
	url = {https://www.taylorfrancis.com/books/9781351460491},
	language = {en},
	urldate = {2022-01-05},
	publisher = {Taylor \& Francis},
	author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
	year = {1984},
	doi = {10.1201/9781315139470},
}

@book{shalev-shwartz_understanding_2014,
	address = {New York, NY, USA},
	title = {Understanding machine learning: from theory to algorithms},
	isbn = {978-1-107-05713-5},
	shorttitle = {Understanding machine learning},
	abstract = {"Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering"--},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	year = {2014},
	keywords = {Algorithms, COMPUTERS / Computer Vision \& Pattern Recognition, Machine learning},
}

@inproceedings{papernot_practical_2017,
	address = {Abu Dhabi United Arab Emirates},
	title = {Practical {Black}-{Box} {Attacks} against {Machine} {Learning}},
	isbn = {978-1-4503-4944-4},
	url = {https://dl.acm.org/doi/10.1145/3052973.3053009},
	doi = {10.1145/3052973.3053009},
	language = {en},
	urldate = {2022-01-05},
	booktitle = {Proceedings of the 2017 {ACM} on {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
	month = apr,
	year = {2017},
	pages = {506--519},
	file = {Eingereichte Version:/Users/ukuhl/Zotero/storage/TKJUNYPC/Papernot et al. - 2017 - Practical Black-Box Attacks against Machine Learni.pdf:application/pdf},
}
